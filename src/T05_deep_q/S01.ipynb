{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc12cbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import random\n",
    "\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "from torchinfo import summary\n",
    "\n",
    "# Only work with scipy 1.12.0\n",
    "# https://discourse.pymc.io/t/importerror-cannot-import-name-gaussian-from-scipy-signal/14170/3\n",
    "from scipy.signal import convolve, gaussian\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccef11ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPISODES_STEPS = 1000\n",
    "\n",
    "\n",
    "def make_env():\n",
    "    env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "    # increase episode length from 200 to 4000\n",
    "    env._max_episode_steps = MAX_EPISODES_STEPS\n",
    "    np.random.seed(13)\n",
    "    env.reset()\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037a0789",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env()\n",
    "plt.imshow(env.render())\n",
    "state_shape, n_actions = env.observation_space.shape, env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34924f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635c83ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent(nn.Module):\n",
    "    def __init__(self, state_shape, n_actions, epsilon=0):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.n_actions = n_actions\n",
    "        self.state_shape = state_shape\n",
    "        state_dim = state_shape[0]\n",
    "        \n",
    "        # a simple NN with state_dim as input vector (inout is state s)    \n",
    "        self.fc1 = nn.Linear(state_dim, 192)\n",
    "        self.fc2 = nn.Linear(192, 256)\n",
    "        self.fc3 = nn.Linear(256, 64)\n",
    "        self.fc4 = nn.Linear(64, n_actions)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # pass the state at time t through the newrok to get Q(s,a)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        qvalues = self.fc4(x)\n",
    "        return qvalues\n",
    "\n",
    "    def get_qvalues(self, states):\n",
    "        # input is an array of states in numpy and outout is Qvals as numpy array\n",
    "        states = torch.tensor(states, device=device, dtype=torch.float32)\n",
    "        qvalues = self.forward(states)\n",
    "        return qvalues.data.cpu().numpy()\n",
    "\n",
    "    def sample_actions(self, qvalues):\n",
    "        # sample actions from a batch of q_values using epsilon greedy policy\n",
    "        epsilon = self.epsilon\n",
    "        batch_size, n_actions = qvalues.shape\n",
    "        random_actions = np.random.choice(n_actions, size=batch_size)\n",
    "        best_actions = qvalues.argmax(axis=-1)\n",
    "        should_explore = np.random.choice([0, 1], batch_size, p=[1 - epsilon, epsilon])\n",
    "        return np.where(should_explore, random_actions, best_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2798da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent(state_shape, n_actions, epsilon=0.5).to(device)\n",
    "summary(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc8d544",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(env, agent, n_games=1, greedy=False, t_max=10000):\n",
    "    rewards = []\n",
    "    for _ in range(n_games):\n",
    "        s, _ = env.reset()\n",
    "        reward = 0\n",
    "        for _ in range(t_max):\n",
    "            qvalues = agent.get_qvalues([s])\n",
    "            action = (\n",
    "                qvalues.argmax(axis=-1)[0]\n",
    "                if greedy\n",
    "                else agent.sample_actions(qvalues)[0]\n",
    "            )\n",
    "            s, r, done, _, _ = env.step(action)\n",
    "            reward += r\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        rewards.append(reward)\n",
    "    return np.mean(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95c199a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(evaluate(env, agent, n_games=1))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713dd653",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, size):\n",
    "        self.size = size  # max number of items in buffer\n",
    "        self.buffer = []  # array to holde buffer\n",
    "        self.next_id = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        item = (state, action, reward, next_state, done)\n",
    "        if len(self.buffer) < self.size:\n",
    "            self.buffer.append(item)\n",
    "        else:\n",
    "            self.buffer[self.next_id] = item\n",
    "        self.next_id = (self.next_id + 1) % self.size\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        idxs = np.random.choice(len(self.buffer), batch_size)\n",
    "        samples = [self.buffer[i] for i in idxs]\n",
    "        states, actions, rewards, next_states, done_flags = list(zip(*samples))\n",
    "        return (\n",
    "            np.array(states),\n",
    "            np.array(actions),\n",
    "            np.array(rewards),\n",
    "            np.array(next_states),\n",
    "            np.array(done_flags),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da6d577",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_and_record(start_state, agent, env, exp_replay, n_steps=1):\n",
    "    s = start_state\n",
    "    sum_rewards = 0\n",
    "\n",
    "    # Play the game for n_steps and record transitions in buffer\n",
    "    for _ in range(n_steps):\n",
    "        qvalues = agent.get_qvalues([s])\n",
    "        a = agent.sample_actions(qvalues)[0]\n",
    "        next_s, r, done, _, _ = env.step(a)\n",
    "        sum_rewards += r\n",
    "        exp_replay.add(s, a, r, next_s, done)\n",
    "        if done:\n",
    "            s, _ = env.reset()\n",
    "        else:\n",
    "            s = next_s\n",
    "\n",
    "    return sum_rewards, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be4f6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_network = DQNAgent(agent.state_shape, agent.n_actions, epsilon=0.5).to(device)\n",
    "target_network.load_state_dict(agent.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52eafbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_td_loss(\n",
    "    agent,\n",
    "    target_network,\n",
    "    states,\n",
    "    actions,\n",
    "    rewards,\n",
    "    next_states,\n",
    "    done_flags,\n",
    "    gamma=0.99,\n",
    "    device=device,\n",
    "):\n",
    "    # convert numpy array to torch tensors\n",
    "    states = torch.tensor(states, device=device, dtype=torch.float)\n",
    "    actions = torch.tensor(actions, device=device, dtype=torch.long)\n",
    "    rewards = torch.tensor(rewards, device=device, dtype=torch.float)\n",
    "    next_states = torch.tensor(next_states, device=device, dtype=torch.float)\n",
    "    done_flags = torch.tensor(\n",
    "        done_flags.astype(\"float32\"), device=device, dtype=torch.float\n",
    "    )\n",
    "\n",
    "    # get q-values for all actions in current states\n",
    "    # use agent network\n",
    "    predicted_qvalues = agent(states)\n",
    "\n",
    "    # compute q-values for all actions in next states\n",
    "    # use target network\n",
    "    predicted_next_qvalues = target_network(next_states)\n",
    "\n",
    "    # select q-values for chosen actions\n",
    "    predicted_qvalues_for_actions = predicted_qvalues[range(len(actions)), actions]\n",
    "\n",
    "    # compute Qmax(next_states, actions) using predicted next q-values\n",
    "    next_state_values, _ = torch.max(predicted_next_qvalues, dim=1)\n",
    "\n",
    "    # compute \"target q-values\"\n",
    "    target_qvalues_for_actions = rewards + gamma * next_state_values * (1 - done_flags)\n",
    "\n",
    "    # mean squared error loss to minimize\n",
    "    loss = torch.mean(\n",
    "        (predicted_qvalues_for_actions - target_qvalues_for_actions.detach()) ** 2\n",
    "    )\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e61fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from tqdm import trange\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a5d08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a seed\n",
    "seed = 13\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a3839e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup env and agent and target networks\n",
    "env = make_env()\n",
    "state_dim = env.observation_space.shape\n",
    "n_actions = env.action_space.n\n",
    "state, _ = env.reset()\n",
    "\n",
    "agent = DQNAgent(state_dim, n_actions, epsilon=1).to(device)\n",
    "target_network = DQNAgent(state_dim, n_actions, epsilon=1).to(device)\n",
    "target_network.load_state_dict(agent.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3006426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us fill experience replay with some samples using full random policy\n",
    "exp_replay = ReplayBuffer(10**4)\n",
    "for i in range(100):\n",
    "    play_and_record(state, agent, env, exp_replay, n_steps=10**2)\n",
    "    if len(exp_replay) == 10**4:\n",
    "        break\n",
    "print(len(exp_replay))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75079692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup some parameters for training\n",
    "timesteps_per_epoch = 1\n",
    "batch_size = 32\n",
    "\n",
    "# init Optimizer\n",
    "opt = torch.optim.Adam(agent.parameters(), lr=1e-4)\n",
    "\n",
    "# set exploration epsilon\n",
    "start_epsilon = 1\n",
    "end_epsilon = 0.05\n",
    "eps_decay_final_step = 2 * 10**4\n",
    "\n",
    "# setup spme frequency for loggind and updating target network\n",
    "loss_freq = 20\n",
    "refresh_target_network_freq = 100\n",
    "eval_freq = 1000\n",
    "\n",
    "# to clip the gradients\n",
    "max_grad_norm = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27e2d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_rw_history = []\n",
    "td_loss_history = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6527d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_schedule(start_eps, end_eps, step, final_step):\n",
    "    return start_eps + (end_eps - start_eps) * min(step, final_step) / final_step\n",
    "\n",
    "\n",
    "def smoothen(values):\n",
    "    kernel = gaussian(100, std=100)\n",
    "    kernel = kernel / np.sum(kernel)\n",
    "    return convolve(values, kernel, \"valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f36c6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "state, _ = env.reset()\n",
    "\n",
    "# You can adjust here\n",
    "total_steps = 20000\n",
    "use_epsilon_schedule = True\n",
    "\n",
    "for step in trange(total_steps + 1):\n",
    "    # reduce exploration as we progress\n",
    "    if use_epsilon_schedule:\n",
    "        agent.epsilon = epsilon_schedule(\n",
    "            start_epsilon, end_epsilon, step, eps_decay_final_step\n",
    "        )\n",
    "        \n",
    "\n",
    "    # take timesteps_per_epoch and update experience replay buffer\n",
    "    _, state = play_and_record(state, agent, env, exp_replay, timesteps_per_epoch)\n",
    "\n",
    "    # train by sampling batch_size of data from experience replay\n",
    "    states, actions, rewards, next_states, done_flags = exp_replay.sample(batch_size)\n",
    "\n",
    "    # loss = <compute TD loss>\n",
    "    loss = compute_td_loss(\n",
    "        agent,\n",
    "        target_network,\n",
    "        states,\n",
    "        actions,\n",
    "        rewards,\n",
    "        next_states,\n",
    "        done_flags,\n",
    "        gamma=0.99,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    loss.backward()\n",
    "    grad_norm = nn.utils.clip_grad_norm_(agent.parameters(), max_grad_norm)\n",
    "    opt.step()\n",
    "    opt.zero_grad()\n",
    "\n",
    "    if step % loss_freq == 0:\n",
    "        td_loss_history.append(loss.data.cpu().item())\n",
    "\n",
    "    if step % refresh_target_network_freq == 0:\n",
    "        # Load agent weights into target_network\n",
    "        target_network.load_state_dict(agent.state_dict())\n",
    "\n",
    "    if step % eval_freq == 0:\n",
    "        # eval the agent\n",
    "        mean_rw_history.append(\n",
    "            evaluate(make_env(), agent, n_games=3, greedy=True, t_max=1000)\n",
    "        )\n",
    "\n",
    "        clear_output(True)\n",
    "        print(\"buffer size = %i, epsilon = %.5f\" % (len(exp_replay), agent.epsilon))\n",
    "\n",
    "        plt.figure(figsize=[16, 5])\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.title(\"Mean reward per episode\")\n",
    "        plt.plot(mean_rw_history)\n",
    "        plt.grid()\n",
    "\n",
    "        assert not np.isnan(td_loss_history[-1])\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.title(\"TD loss history (smoothened)\")\n",
    "        plt.plot(smoothen(td_loss_history))\n",
    "        plt.grid()\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d110f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()\n",
    "env = make_env()\n",
    "timestamp = int(datetime.datetime.now().timestamp())\n",
    "timestamp = int(datetime.datetime.now().timestamp())\n",
    "env = RecordVideo(env=env, video_folder=\"./video\", name_prefix=timestamp)\n",
    "state, _ = env.reset()\n",
    "reward = 0\n",
    "idx = 0\n",
    "while True:\n",
    "    qvalues = agent.get_qvalues([state])\n",
    "    action = qvalues.argmax(axis=-1)[0]\n",
    "    state, r, done, _, _ = env.step(action)\n",
    "    reward += r\n",
    "    if done or idx > MAX_EPISODES_STEPS:\n",
    "        print(\"Got reward: {}\".format(reward))\n",
    "        break\n",
    "    idx += 1\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-training-2025-codes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
