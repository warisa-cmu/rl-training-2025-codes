{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b382c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from gridworld import GridworldEnv\n",
    "\n",
    "env = GridworldEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cf1197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Greedy in the Limit with Infinite Exploration (GLIE)\n",
    "def GLIE(env, discount_factor=1.0, episode_count=100):\n",
    "    \"\"\"\n",
    "    Find optimal policy given an environment.\n",
    "\n",
    "    Args:\n",
    "        env: OpenAI env. In model free setup you have no access to env.P,\n",
    "             transition dynamics of the environment.\n",
    "             use step(a) to take an action and receive a tuple\n",
    "             of (s', r, done, info)\n",
    "             env.nS is number of states in the environment.\n",
    "             env.nA is number of actions in the environment.\n",
    "        episode_count: Number of episodes:\n",
    "        discount_factor: Gamma discount factor.\n",
    "\n",
    "    Returns:\n",
    "        Vector of length env.nS representing the value function.\n",
    "        policy:[S, A] shaped matrix representing the policy. Random in our case\n",
    "\n",
    "    \"\"\"\n",
    "    # Start with (all 0) state value array and state-action matrix.\n",
    "    # also initialize visit count to zero for the state-action visit count.\n",
    "    V = np.zeros(env.nS)\n",
    "    N = np.zeros((env.nS, env.nA))\n",
    "    Q = np.zeros((env.nS, env.nA))\n",
    "    # random policy\n",
    "    policy = [np.random.randint(env.nA) for _ in range(env.nS)]\n",
    "    k = 1\n",
    "    eps = 1\n",
    "\n",
    "    def argmax_a(arr):\n",
    "        \"\"\"\n",
    "        Return idx of max element in an array.\n",
    "        Break ties uniformly.\n",
    "        \"\"\"\n",
    "        max_idx = []\n",
    "        max_val = float(\"-inf\")\n",
    "        for idx, elem in enumerate(arr):\n",
    "            if elem == max_val:\n",
    "                max_idx.append(idx)\n",
    "            elif elem > max_val:\n",
    "                max_idx = [idx]\n",
    "                max_val = elem\n",
    "        return np.random.choice(max_idx)\n",
    "\n",
    "    def get_action(state):\n",
    "        if np.random.random() < eps:\n",
    "            return np.random.choice(env.nA)\n",
    "        else:\n",
    "            return argmax_a(Q[state])\n",
    "\n",
    "    # run multiple episodes\n",
    "    while k <= episode_count:\n",
    "        # collect samples for one episode\n",
    "        episode_states = []\n",
    "        episode_actions = []\n",
    "        episode_returns = []\n",
    "        state, _ = env.reset()\n",
    "        episode_states.append(state)\n",
    "        while True:\n",
    "            action = get_action(state)\n",
    "            episode_actions.append(action)\n",
    "            (state, reward, done, _, _) = env.step(action)\n",
    "            episode_returns.append(reward)\n",
    "            if not done:\n",
    "                episode_states.append(state)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        # update state-action values\n",
    "        G = 0\n",
    "        count = len(episode_states)\n",
    "        for t in range(count - 1, -1, -1):\n",
    "            s, a, r = episode_states[t], episode_actions[t], episode_returns[t]\n",
    "            G = discount_factor * G + r\n",
    "            N[s, a] += 1\n",
    "            Q[s, a] = Q[s, a] + 1 / N[s, a] * (G - Q[s, a])\n",
    "\n",
    "        # Update policy and optimal value\n",
    "        k = k + 1\n",
    "        eps = 1 / k\n",
    "        # uncomment \"if\" to have higher exploration initially and\n",
    "        # then let epislon decay after 5000 episodes\n",
    "        # if k <=100:\n",
    "        #    eps = 0.02\n",
    "\n",
    "    for s in range(env.nS):\n",
    "        best_action = argmax_a(Q[s])\n",
    "        policy[s] = best_action\n",
    "        V[s] = Q[s, best_action]\n",
    "\n",
    "    return np.array(V), np.array(policy), Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf186e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom print to show state values inside the grid\n",
    "def grid_print(V):\n",
    "    ax = sns.heatmap(\n",
    "        V.reshape(env.shape),\n",
    "        annot=True,\n",
    "        square=True,\n",
    "        cbar=False,\n",
    "        cmap=\"Blues\",\n",
    "        xticklabels=False,\n",
    "        yticklabels=False,\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17072abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run mc policy control GLIE\n",
    "V_pi, policy, Q_pi = GLIE(env, discount_factor=1.0, episode_count=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce6a396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print policy\n",
    "action_labels = {0: \"UP\", 1: \"RIGHT\", 2: \"DOWN\", 3: \"LEFT\"}\n",
    "optimal_actions = [action_labels[policy[s]] for s in range(env.nS)]\n",
    "optimal_actions[0] = \"*\"\n",
    "optimal_actions[-1] = \"*\"\n",
    "print(policy)\n",
    "print(optimal_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d4808d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print state values\n",
    "grid_print(V_pi.reshape(env.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc6d9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print Q\n",
    "print(Q_pi)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-training-2025-codes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
