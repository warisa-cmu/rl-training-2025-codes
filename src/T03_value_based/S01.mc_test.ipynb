{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f23ae17",
   "metadata": {},
   "source": [
    "# Monte Carlo Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c68cdfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcf77df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the policy for choosing paths starting from state \"A\"\n",
    "# pi_A[0]: Probability of choosing path \"A-B-C\"\n",
    "# pi_A[1]: Probability of choosing path \"A-C\"\n",
    "pi_A = np.array([1 / 3, 2 / 3])\n",
    "\n",
    "# Total number of episodes (experiments) to run\n",
    "num_episode = 20000\n",
    "\n",
    "# Array to store the return (reward) obtained in each episode\n",
    "G_array = np.zeros((num_episode,))\n",
    "\n",
    "# Run the Monte Carlo simulation\n",
    "for idx in range(num_episode):\n",
    "    # Randomly select a path (\"A-B-C\" or \"A-C\") based on the policy probabilities\n",
    "    path = np.random.choice([\"A-B-C\", \"A-C\"], p=pi_A)\n",
    "\n",
    "    # Assign rewards based on the chosen path:\n",
    "    # If the path is \"A-B-C\", the reward is 2; else, it's 1 for \"A-C\"\n",
    "    if path == \"A-B-C\":\n",
    "        reward = 2\n",
    "    else:\n",
    "        reward = 1\n",
    "\n",
    "    # Store the obtained reward for this episode\n",
    "    G_array[idx] = reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d54a4346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.33495\n"
     ]
    }
   ],
   "source": [
    "# Estimate the value of state \"A\" by averaging returns (first-visit MC estimate)\n",
    "v_a = G_array.sum() / num_episode\n",
    "print(v_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac30cc5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3349499999999959\n"
     ]
    }
   ],
   "source": [
    "# Now, estimate the value using incremental (online/recursive) mean calculation\n",
    "v_a_updated = 0\n",
    "for idx in range(num_episode):\n",
    "    G = G_array[idx]\n",
    "    # Update the running mean (average) with the return from the current episode\n",
    "    # v_a_updated = prev_mean + (1/N) * (current_value - prev_mean)\n",
    "    v_a_updated = v_a_updated + 1 / (idx + 1) * (G - v_a_updated)\n",
    "\n",
    "print(v_a_updated)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-training-2025-codes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
