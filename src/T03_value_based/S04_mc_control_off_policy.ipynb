{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bb24fea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from gridworld import GridworldEnv\n",
    "from collections import defaultdict\n",
    "\n",
    "env = GridworldEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "38e9e575",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_behaviour_policy(nA):\n",
    "    def policy_fn(observation):\n",
    "        A = np.ones(nA, dtype=float) / nA\n",
    "        return A\n",
    "\n",
    "    return policy_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4a4a5729",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_target_policy(Q):\n",
    "    def policy_fn(state):\n",
    "        A = np.zeros_like(Q[state], dtype=float)\n",
    "        best_action = np.argmax(Q[state])\n",
    "        A[best_action] = 1.0\n",
    "        return A\n",
    "\n",
    "    return policy_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "65911bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_control_off_policy_importance_sampling(\n",
    "    behaviour_policy, env, num_episodes, discount=1.0, debug=False\n",
    "):\n",
    "    \n",
    "    def argmax_a(arr):\n",
    "        \"\"\"\n",
    "        Return idx of max element in an array.\n",
    "        Break ties uniformly.\n",
    "        \"\"\"\n",
    "        max_idx = []\n",
    "        max_val = float(\"-inf\")\n",
    "        for idx, elem in enumerate(arr):\n",
    "            if elem == max_val:\n",
    "                max_idx.append(idx)\n",
    "            elif elem > max_val:\n",
    "                max_idx = [idx]\n",
    "                max_val = elem\n",
    "        return np.random.choice(max_idx)\n",
    "    \n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    C = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "\n",
    "    target_policy = create_target_policy(Q)\n",
    "\n",
    "    for i_episode in range(1, num_episodes + 1):\n",
    "        if debug:\n",
    "            if i_episode % 100000 == 0:\n",
    "                print(\"\\rEpisode {}/{}.\".format(i_episode, num_episodes))\n",
    "\n",
    "        state, _ = env.reset()\n",
    "        episode = []\n",
    "        while True:\n",
    "            probs = behaviour_policy(state)\n",
    "            action = np.random.choice(len(probs), p=probs)\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            episode.append((state, action, reward))\n",
    "            if done:\n",
    "                break\n",
    "            state = next_state\n",
    "\n",
    "        G = 0.0\n",
    "        W = 1.0\n",
    "\n",
    "        for t in range(len(episode))[::-1]:\n",
    "            state, action, reward = episode[t]\n",
    "            G = discount * G + reward\n",
    "            C[state][action] += W\n",
    "            Q[state][action] += (W / C[state][action]) * (G - Q[state][action])\n",
    "\n",
    "            if action != np.argmax(target_policy(state)):\n",
    "                break\n",
    "\n",
    "            W = W * (target_policy(state)[action] / behaviour_policy(state)[action])\n",
    "   \n",
    "    V = np.zeros(env.nS)\n",
    "    policy = [np.zeros(env.nA) for _ in range(env.nS)]\n",
    "    for s in range(env.nS):\n",
    "        best_action = argmax_a(Q[s])\n",
    "        policy[s] = best_action\n",
    "        V[s] = Q[s][best_action]\n",
    "        \n",
    "        \n",
    "    return Q, target_policy, V, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a13c7292",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m behaviour_policy = create_behaviour_policy(env.action_space.n)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m optimal_Q, optimal_policy, V, policy = \u001b[43mmc_control_off_policy_importance_sampling\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbehaviour_policy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m      4\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36mmc_control_off_policy_importance_sampling\u001b[39m\u001b[34m(behaviour_policy, env, num_episodes, discount, debug)\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m     33\u001b[39m     probs = behaviour_policy(state)\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m     action = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrandom\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m     next_state, reward, done, _, _ = env.step(action)\n\u001b[32m     36\u001b[39m     episode.append((state, action, reward))\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "behaviour_policy = create_behaviour_policy(env.action_space.n)\n",
    "optimal_Q, optimal_policy, V, policy = mc_control_off_policy_importance_sampling(\n",
    "    behaviour_policy, env, num_episodes=10000, debug=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e51f42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(3),\n",
       " np.int64(0),\n",
       " np.int64(2),\n",
       " np.int64(1),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(1),\n",
       " np.int64(2),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(2)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38cc6bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963300af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.mc_control_off_policy_importance_sampling.<locals>.<lambda>()>,\n",
       "            {np.int64(4): array([-1.,  0.,  0.,  0.]),\n",
       "             np.int64(11): array([ 0.,  0., -1.,  0.]),\n",
       "             np.int64(14): array([ 0., -1.,  0.,  0.]),\n",
       "             np.int64(1): array([ 0.,  0.,  0., -1.]),\n",
       "             0: array([0., 0., 0., 0.]),\n",
       "             2: array([0., 0., 0., 0.]),\n",
       "             3: array([0., 0., 0., 0.]),\n",
       "             5: array([0., 0., 0., 0.]),\n",
       "             6: array([0., 0., 0., 0.]),\n",
       "             7: array([0., 0., 0., 0.]),\n",
       "             8: array([0., 0., 0., 0.]),\n",
       "             9: array([0., 0., 0., 0.]),\n",
       "             10: array([0., 0., 0., 0.]),\n",
       "             12: array([0., 0., 0., 0.]),\n",
       "             13: array([0., 0., 0., 0.]),\n",
       "             15: array([0., 0., 0., 0.])})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b880b6a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-training-2025-codes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
