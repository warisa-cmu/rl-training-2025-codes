{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46529529",
   "metadata": {},
   "source": [
    "# Semi Gradient n-step SARSA\n",
    "> Mountain Car Problem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c0c7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e602c1d1",
   "metadata": {},
   "source": [
    "## Env\n",
    "\n",
    "https://gymnasium.farama.org/environments/classic_control/mountain_car/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75e2329",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MountainCar-v0\", render_mode=\"rgb_array\")\n",
    "\n",
    "# increase episode length from 200 to 4000\n",
    "env._max_episode_steps = 4000\n",
    "\n",
    "np.random.seed(13)\n",
    "env.reset()\n",
    "plt.imshow(env.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6797b457",
   "metadata": {},
   "outputs": [],
   "source": [
    "state, _ = env.reset()\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b930150",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2faae10",
   "metadata": {},
   "source": [
    "## Random Action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75209deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_reward = 0.0\n",
    "total_steps = 0\n",
    "timestamp = int(datetime.datetime.now().timestamp())\n",
    "env = RecordVideo(env=env, video_folder=\"./video\", name_prefix=timestamp)\n",
    "obs, _ = env.reset()\n",
    "\n",
    "done = False\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    total_steps += 1\n",
    "    done = terminated or truncated\n",
    "    if total_steps % 200 == 0:\n",
    "        print(\n",
    "            f\"Step {total_steps:2d}: action={action:2d}, Reward={reward:5.2f}, \"\n",
    "            f\"Terminated={terminated}, Truncated={truncated}, info={info}\"\n",
    "        )\n",
    "\n",
    "print(\n",
    "    f\"Episode finished after {total_steps} steps with total reward: {total_reward:.2f}\"\n",
    ")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9958e6e6",
   "metadata": {},
   "source": [
    "## Reinforcement Learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2795af40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The implementation of tiles3 is from Richard Sutton's website\n",
    "# http://incompleteideas.net/tiles/tiles3.html\n",
    "from tiles3 import IHT, tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3054562d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QEstimator:\n",
    "    def __init__(\n",
    "        self, step_size, num_of_tilings=8, tiles_per_dim=8, max_size=2048, epsilon=0.0\n",
    "    ):\n",
    "        self.max_size = max_size\n",
    "        self.num_of_tilings = num_of_tilings\n",
    "        self.tiles_per_dim = tiles_per_dim\n",
    "        self.epsilon = epsilon\n",
    "        self.step_size = step_size / num_of_tilings\n",
    "\n",
    "        self.table = IHT(max_size)\n",
    "\n",
    "        self.w = np.zeros(max_size)\n",
    "\n",
    "        self.pos_scale = self.tiles_per_dim / (\n",
    "            env.observation_space.high[0] - env.observation_space.low[0]\n",
    "        )\n",
    "        self.vel_scale = self.tiles_per_dim / (\n",
    "            env.observation_space.high[1] - env.observation_space.low[1]\n",
    "        )\n",
    "\n",
    "    def get_active_features(self, state, action):\n",
    "        pos, vel = state\n",
    "        active_features = tiles(\n",
    "            self.table,\n",
    "            self.num_of_tilings,\n",
    "            [\n",
    "                self.pos_scale * (pos - env.observation_space.low[0]),\n",
    "                self.vel_scale * (vel - env.observation_space.low[1]),\n",
    "            ],\n",
    "            [action],\n",
    "        )\n",
    "        return active_features\n",
    "\n",
    "    def q_predict(self, state, action):\n",
    "        pos, vel = state\n",
    "        if pos == env.observation_space.high[0]:  # reached goal\n",
    "            return 0.0\n",
    "        else:\n",
    "            active_features = self.get_active_features(state, action)\n",
    "            return np.sum(self.w[active_features])\n",
    "\n",
    "    # learn with given state, action and target\n",
    "    def q_update(self, state, action, target):\n",
    "        active_features = self.get_active_features(state, action)\n",
    "        q_s_a = np.sum(self.w[active_features])\n",
    "        delta = target - q_s_a\n",
    "        self.w[active_features] += self.step_size * delta\n",
    "\n",
    "    def get_eps_greedy_action(self, state):\n",
    "        pos, vel = state\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.choice(env.action_space.n)\n",
    "        else:\n",
    "            qvals = np.array(\n",
    "                [self.q_predict(state, action) for action in range(env.action_space.n)]\n",
    "            )\n",
    "            return np.argmax(qvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f9db01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa_n(qhat, env, n=1, gamma=1.0, episode_cnt=10000):\n",
    "    episode_rewards = []\n",
    "    print_every = max(1, episode_cnt // 10)\n",
    "    for ec in range(episode_cnt):\n",
    "        if ec % print_every == 0 or ec == episode_cnt - 1:\n",
    "            print(f\"Episode: {ec + 1}/{episode_cnt}\")\n",
    "        state, _ = env.reset()\n",
    "        action = qhat.get_eps_greedy_action(state)\n",
    "        T = float(\"inf\")\n",
    "        t = 0\n",
    "        states = [state]\n",
    "        actions = [action]\n",
    "        rewards = [0.0]\n",
    "        while True:\n",
    "            if t < T:\n",
    "                next_state, reward, done, _, _ = env.step(action)\n",
    "                states.append(next_state)\n",
    "                rewards.append(reward)\n",
    "\n",
    "                if done:\n",
    "                    T = t + 1\n",
    "                else:\n",
    "                    next_action = qhat.get_eps_greedy_action(next_state)\n",
    "                    actions.append(next_action)\n",
    "\n",
    "            tau = t - n + 1\n",
    "\n",
    "            if tau >= 0:\n",
    "                G = 0\n",
    "                for i in range(tau + 1, min(tau + n, T) + 1):\n",
    "                    G += gamma ** (i - tau - 1) * rewards[i]\n",
    "                if tau + n < T:\n",
    "                    G += gamma**n * qhat.q_predict(states[tau + n], actions[tau + n])\n",
    "                qhat.q_update(states[tau], actions[tau], G)\n",
    "\n",
    "            if tau == T - 1:\n",
    "                episode_rewards.append(np.sum(rewards))\n",
    "                break\n",
    "            else:\n",
    "                t += 1\n",
    "                state = next_state\n",
    "                action = next_action\n",
    "\n",
    "    return np.array(episode_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa27a0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot rewards\n",
    "def plot_rewards(env_name, rewards, label):\n",
    "    plt.title(\"env={}, Mean reward = {:.1f}\".format(env_name, np.mean(rewards[-20:])))\n",
    "    plt.plot(rewards, label=label)\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.ylim(-500, 0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1591962c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize environment\n",
    "env = gym.make(\"MountainCar-v0\", render_mode=\"rgb_array\")\n",
    "state, _ = env.reset(seed=13)\n",
    "print(state)\n",
    "\n",
    "# create a n-SARSA Learning agent\n",
    "step_size = 0.8\n",
    "episode_cnt = 1000\n",
    "n = 4\n",
    "epsilon = 0.01\n",
    "gamma = 1.0\n",
    "estimator = QEstimator(step_size, epsilon=epsilon)\n",
    "reward_arr = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4a2b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_cnt = 100\n",
    "\n",
    "rewards = sarsa_n(\n",
    "    estimator,\n",
    "    env=env,\n",
    "    n=n,\n",
    "    gamma=gamma,\n",
    "    episode_cnt=episode_cnt,\n",
    ")\n",
    "reward_arr = [*reward_arr, *rewards]\n",
    "\n",
    "# plot rewards\n",
    "plot_rewards(\"Mountain Car World\", reward_arr, \"Semi Grad n-step SARSA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1fb3ee",
   "metadata": {},
   "source": [
    "### Take a look into the agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e71ae81",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.observation_space.sample()\n",
    "action = env.action_space.sample()\n",
    "print(\"=\" * 40)\n",
    "print(f\"Sampled state:  {state}\")\n",
    "print(f\"Sampled action: {action}\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "active_features = estimator.get_active_features(state=state, action=action)\n",
    "print(\"Active features for selected state-action pair:\")\n",
    "print(active_features)\n",
    "print(\"=\" * 40)\n",
    "\n",
    "q_pred = [estimator.q_predict(state=state, action=a) for a in range(env.action_space.n)]\n",
    "print(\"Q-value predictions for all actions:\")\n",
    "for idx, q in enumerate(q_pred):\n",
    "    print(f\"  Action {idx}: {q:.3f}\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "a_greedy = estimator.get_eps_greedy_action(state=state)\n",
    "print(f\"Epsilon-greedy action for sampled state: {a_greedy}\")\n",
    "print(\"=\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338f1d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df655e47",
   "metadata": {},
   "source": [
    "## Take a look at the result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cbfdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MountainCar-v0\", render_mode=\"rgb_array\")\n",
    "timestamp = int(datetime.datetime.now().timestamp())\n",
    "timestamp = int(datetime.datetime.now().timestamp())\n",
    "env = RecordVideo(env=env, video_folder=\"./video\", name_prefix=timestamp)\n",
    "rewards = sarsa_n(\n",
    "    estimator,\n",
    "    env=env,\n",
    "    n=n,\n",
    "    gamma=gamma,\n",
    "    episode_cnt=1,\n",
    ")\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-training-2025-codes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
